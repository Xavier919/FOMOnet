{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0454852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorboard\n",
    "from modules.transcripts import Transcripts\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from modules.model import FOMOnet\n",
    "from modules.lrp import LRP_FOMOnet\n",
    "from modules.utils import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c7e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2713e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = LRP_FOMOnet()\n",
    "input_data = torch.randn(1, 4, 1000)  # Example input with shape (batch_size, num_channels, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd6bea20",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6243, 0.6243, 0.6243, 0.4150, 0.4150, 0.5256, 0.5256, 0.7172,\n",
      "          0.7172, 0.5438, 0.5438, 0.5438, 0.2740, 0.2740, 0.2918, 0.2918,\n",
      "          0.8202, 0.8202, 0.3225, 0.3225, 0.3225, 0.8965, 0.8965, 0.6363,\n",
      "          0.6363, 0.6699, 0.6699, 0.7900, 0.7900, 0.7900, 0.3507, 0.3507,\n",
      "          0.8721, 0.8721, 0.3027, 0.3027, 0.4064, 0.4064, 0.3514, 0.3514,\n",
      "          0.3514, 0.5414, 0.5414, 0.2146, 0.2146, 0.4702, 0.4702, 0.3132,\n",
      "          0.3132, 0.3132, 0.3802, 0.3802, 0.3456, 0.3456, 0.3219, 0.3219,\n",
      "          0.6241, 0.6241, 0.6241, 0.7145, 0.7145, 0.3808, 0.3808, 0.6249,\n",
      "          0.6249, 0.6648, 0.6648, 0.3393, 0.3393, 0.3393, 0.3644, 0.3644,\n",
      "          0.5011, 0.5011, 0.3421, 0.3421, 0.5361, 0.5361, 0.5361, 0.3639,\n",
      "          0.3639, 0.5889, 0.5889, 0.3987, 0.3987, 0.7356, 0.7356, 0.7356,\n",
      "          0.3575, 0.3575, 0.2537, 0.2537, 0.8715, 0.8715, 0.2844, 0.2844,\n",
      "          0.8434, 0.8434, 0.8434, 0.3524, 0.3524, 0.3357, 0.3357, 0.5335,\n",
      "          0.5335, 0.6796, 0.6796, 0.6796, 0.7391, 0.7391, 0.4617, 0.4617,\n",
      "          0.4829, 0.4829, 0.6396, 0.6396, 0.6396, 0.4037, 0.4037, 0.6401,\n",
      "          0.6401, 0.3420, 0.3420, 0.3852, 0.3852, 0.3118, 0.3118, 0.3118,\n",
      "          0.3909, 0.3909, 0.3217, 0.3217, 0.4055, 0.4055, 0.2382, 0.2382,\n",
      "          0.2382, 0.2665, 0.2665, 0.9631, 0.9631, 0.3595, 0.3595, 0.7882,\n",
      "          0.7882, 0.7882, 0.1694, 0.1694, 0.5689, 0.5689, 0.9375, 0.9375,\n",
      "          0.9580, 0.9580, 0.9580, 0.2476, 0.2476, 0.3210, 0.3210, 0.7930,\n",
      "          0.7930, 0.6809, 0.6809, 0.2944, 0.2944, 0.2944, 0.2428, 0.2428,\n",
      "          0.3493, 0.3493, 0.2417, 0.2417, 0.3443, 0.3443, 0.3443, 0.4807,\n",
      "          0.4807, 0.2567, 0.2567, 0.3605, 0.3605, 0.3315, 0.3315, 0.3315,\n",
      "          0.3033, 0.3033, 0.9389, 0.9389, 0.6337, 0.6337, 0.5755, 0.5755,\n",
      "          0.9863, 0.9863, 0.9863, 0.5590, 0.5590, 0.3115, 0.3115, 0.8771,\n",
      "          0.8771, 0.9612, 0.9612, 0.9612, 0.2704, 0.2704, 0.4861, 0.4861,\n",
      "          0.7518, 0.7518, 0.8161, 0.8161, 0.8161, 0.4030, 0.4030, 0.5199,\n",
      "          0.5199, 0.4144, 0.4144, 0.3066, 0.3066, 0.3533, 0.3533, 0.3533,\n",
      "          0.7097, 0.7097, 0.4483, 0.4483, 0.3431, 0.3431, 0.3371, 0.3371,\n",
      "          0.3371, 0.3647, 0.3647, 0.7344, 0.7344, 0.3460, 0.3460, 0.4616,\n",
      "          0.4616, 0.4616, 0.4458, 0.4458, 0.3601, 0.3601, 0.4964, 0.4964,\n",
      "          0.4134, 0.4134, 0.3671, 0.3671, 0.3671, 0.5640, 0.5640, 0.5899,\n",
      "          0.5899, 0.6956, 0.6956, 0.5096, 0.5096, 0.5096, 0.3484, 0.3484,\n",
      "          0.5825, 0.5825, 0.5080, 0.5080, 0.4935, 0.4935, 0.4935, 0.3212,\n",
      "          0.3212, 0.3807, 0.3807, 0.3600, 0.3600, 0.3032, 0.3032, 0.3032,\n",
      "          0.5543, 0.5543, 0.3434, 0.3434, 0.4340, 0.4340, 0.3305, 0.3305,\n",
      "          0.6004, 0.6004, 0.6004, 0.3472, 0.3472, 0.3236, 0.3236, 0.5043,\n",
      "          0.5043, 0.8295, 0.8295, 0.8295, 0.3812, 0.3812, 0.4922, 0.4922,\n",
      "          0.5827, 0.5827, 0.2913, 0.2913, 0.2913, 0.2525, 0.2525, 0.3903,\n",
      "          0.3903, 0.4562, 0.4562, 0.4270, 0.4270, 0.3301, 0.3301, 0.3301,\n",
      "          0.7617, 0.7617, 0.5344, 0.5344, 0.2995, 0.2995, 0.3614, 0.3614,\n",
      "          0.3614, 0.3508, 0.3508, 0.4436, 0.4436, 0.5216, 0.5216, 0.2209,\n",
      "          0.2209, 0.2209, 0.8721, 0.8721, 0.3356, 0.3356, 0.3486, 0.3486,\n",
      "          0.3171, 0.3171, 0.4377, 0.4377, 0.4377, 0.8545, 0.8545, 0.2561,\n",
      "          0.2561, 0.3124, 0.3124, 0.5423, 0.5423, 0.5423, 0.7015, 0.7015,\n",
      "          0.6818, 0.6818, 0.3439, 0.3439, 0.5661, 0.5661, 0.5661, 0.5329,\n",
      "          0.5329, 0.5669, 0.5669, 0.2768, 0.2768, 0.2378, 0.2378, 0.3644,\n",
      "          0.3644, 0.3644, 0.3062, 0.3062, 0.2635, 0.2635, 0.5984, 0.5984,\n",
      "          0.8802, 0.8802, 0.8802, 0.4985, 0.4985, 0.7406, 0.7406, 0.5680,\n",
      "          0.5680, 0.8830, 0.8830, 0.8830, 0.6282, 0.6282, 0.3341, 0.3341,\n",
      "          0.3771, 0.3771, 0.5067, 0.5067, 0.5067, 0.4711, 0.4711, 0.2713,\n",
      "          0.2713, 0.3513, 0.3513, 0.5748, 0.5748, 0.2982, 0.2982, 0.2982,\n",
      "          0.2622, 0.2622, 0.7456, 0.7456, 0.3078, 0.3078, 0.6967, 0.6967,\n",
      "          0.6967, 0.9534, 0.9534, 0.9407, 0.9407, 0.3251, 0.3251, 0.4952,\n",
      "          0.4952, 0.4952, 0.2683, 0.2683, 0.5928, 0.5928, 0.2916, 0.2916,\n",
      "          0.2576, 0.2576, 0.8630, 0.8630, 0.8630, 0.3282, 0.3282, 0.8680,\n",
      "          0.8680, 0.5038, 0.5038, 0.3403, 0.3403, 0.3403, 0.3549, 0.3549,\n",
      "          0.3320, 0.3320, 0.5557, 0.5557, 0.3250, 0.3250, 0.3250, 0.3390,\n",
      "          0.3390, 0.6345, 0.6345, 0.3493, 0.3493, 0.2885, 0.2885, 0.4016,\n",
      "          0.4016, 0.4016, 0.8160, 0.8160, 0.3632, 0.3632, 0.3621, 0.3621,\n",
      "          0.3809, 0.3809, 0.3809, 0.2676, 0.2676, 0.3122, 0.3122, 0.2943,\n",
      "          0.2943, 0.7277, 0.7277, 0.7277, 0.7072, 0.7072, 0.1806, 0.1806,\n",
      "          0.6639, 0.6639, 0.3473, 0.3473, 0.2751, 0.2751, 0.2751, 0.4056,\n",
      "          0.4056, 0.6566, 0.6566, 0.3615, 0.3615, 0.4368, 0.4368, 0.4368,\n",
      "          0.5746, 0.5746, 0.7429, 0.7429, 0.3682, 0.3682, 0.5668, 0.5668,\n",
      "          0.5668, 0.2874, 0.2874, 0.5635, 0.5635, 0.3168, 0.3168, 0.8429,\n",
      "          0.8429, 0.8429, 0.3063, 0.3063, 0.8094, 0.8094, 0.9386, 0.9386,\n",
      "          0.4597, 0.4597, 0.2158, 0.2158, 0.2158, 0.8364, 0.8364, 0.3696,\n",
      "          0.3696, 0.6373, 0.6373, 0.7679, 0.7679, 0.7679, 0.3933, 0.3933,\n",
      "          0.3233, 0.3233, 0.4489, 0.4489, 0.5899, 0.5899, 0.5899, 0.7650,\n",
      "          0.7650, 0.7378, 0.7378, 0.8007, 0.8007, 0.2888, 0.2888, 0.3534,\n",
      "          0.3534, 0.3534, 0.3594, 0.3594, 0.4807, 0.4807, 0.6380, 0.6380,\n",
      "          0.2903, 0.2903, 0.2903, 0.4674, 0.4674, 0.8139, 0.8139, 0.3315,\n",
      "          0.3315, 0.5957, 0.5957, 0.5957, 0.3425, 0.3425, 0.4372, 0.4372,\n",
      "          0.3292, 0.3292, 0.3313, 0.3313, 0.2972, 0.2972, 0.2972, 0.3112,\n",
      "          0.3112, 0.6974, 0.6974, 0.6291, 0.6291, 0.5300, 0.5300, 0.5300,\n",
      "          0.4189, 0.4189, 0.4055, 0.4055, 0.5028, 0.5028, 0.2635, 0.2635,\n",
      "          0.2635, 0.3426, 0.3426, 0.3585, 0.3585, 0.5203, 0.5203, 0.4651,\n",
      "          0.4651, 0.5170, 0.5170, 0.5170, 0.5963, 0.5963, 0.3047, 0.3047,\n",
      "          0.4529, 0.4529, 0.4651, 0.4651, 0.4651, 0.3180, 0.3180, 0.6450,\n",
      "          0.6450, 0.1424, 0.1424, 0.2854, 0.2854, 0.2854, 0.2998, 0.2998,\n",
      "          0.5449, 0.5449, 0.3252, 0.3252, 0.7551, 0.7551, 0.7551, 0.3360,\n",
      "          0.3360, 0.8865, 0.8865, 0.6450, 0.6450, 0.7473, 0.7473, 0.8083,\n",
      "          0.8083, 0.8083, 0.4396, 0.4396, 0.4231, 0.4231, 0.4817, 0.4817,\n",
      "          0.4746, 0.4746, 0.4746, 0.6648, 0.6648, 0.3051, 0.3051, 0.2921,\n",
      "          0.2921, 0.7712, 0.7712, 0.7712, 0.7043, 0.7043, 0.7659, 0.7659,\n",
      "          0.7250, 0.7250, 0.4082, 0.4082, 0.3275, 0.3275, 0.3275, 0.7489,\n",
      "          0.7489, 0.3020, 0.3020, 0.3879, 0.3879, 0.3212, 0.3212, 0.3212,\n",
      "          0.4630, 0.4630, 0.8002, 0.8002, 0.9617, 0.9617, 0.5267, 0.5267,\n",
      "          0.5267, 0.3160, 0.3160, 0.3230, 0.3230, 0.3560, 0.3560, 0.2957,\n",
      "          0.2957, 0.5514, 0.5514, 0.5514, 0.6280, 0.6280, 0.3048, 0.3048,\n",
      "          0.6278, 0.6278, 0.4403, 0.4403, 0.4403, 0.4444, 0.4444, 0.3793,\n",
      "          0.3793, 0.3162, 0.3162, 0.5259, 0.5259, 0.5259, 0.3624, 0.3624,\n",
      "          0.4086, 0.4086, 0.3459, 0.3459, 0.3376, 0.3376, 0.5411, 0.5411,\n",
      "          0.5411, 0.2699, 0.2699, 0.2907, 0.2907, 0.2614, 0.2614, 0.3187,\n",
      "          0.3187, 0.3187, 0.3328, 0.3328, 0.3655, 0.3655, 0.2367, 0.2367,\n",
      "          0.5857, 0.5857, 0.5857, 0.6922, 0.6922, 0.3246, 0.3246, 0.5788,\n",
      "          0.5788, 0.4217, 0.4217, 0.4217, 0.3303, 0.3303, 0.7110, 0.7110,\n",
      "          0.5291, 0.5291, 0.3391, 0.3391, 0.7240, 0.7240, 0.7240, 0.6737,\n",
      "          0.6737, 0.4405, 0.4405, 0.5110, 0.5110, 0.2725, 0.2725, 0.2725,\n",
      "          0.3253, 0.3253, 0.6550, 0.6550, 0.4922, 0.4922, 0.6305, 0.6305,\n",
      "          0.6305, 0.4826, 0.4826, 0.4988, 0.4988, 0.8496, 0.8496, 0.7609,\n",
      "          0.7609, 0.3709, 0.3709, 0.3709, 0.6578, 0.6578, 0.3465, 0.3465,\n",
      "          0.3469, 0.3469, 0.2938, 0.2938, 0.2938, 0.3602, 0.3602, 0.3635,\n",
      "          0.3635, 0.2256, 0.2256, 0.3441, 0.3441, 0.3441, 0.4977, 0.4977,\n",
      "          0.2752, 0.2752, 0.3152, 0.3152, 0.2930, 0.2930, 0.8614, 0.8614,\n",
      "          0.8614, 0.9657, 0.9657, 0.5120, 0.5120, 0.6702, 0.6702, 0.7897,\n",
      "          0.7897, 0.7897, 0.2788, 0.2788, 0.2875, 0.2875, 0.3127, 0.3127,\n",
      "          0.4198, 0.4198, 0.4198, 0.6057, 0.6057, 0.3653, 0.3653, 0.4968,\n",
      "          0.4968, 0.9766, 0.9766, 0.3250, 0.3250, 0.3250, 0.3599, 0.3599,\n",
      "          0.3278, 0.3278, 0.2261, 0.2261, 0.4262, 0.4262, 0.4262, 0.5279,\n",
      "          0.5279, 0.9388, 0.9388, 0.6610, 0.6610, 0.5103, 0.5103, 0.5103,\n",
      "          0.2787, 0.2787, 0.3431, 0.3431, 0.8800, 0.8800, 0.2940, 0.2940,\n",
      "          0.2940, 0.3387, 0.3387, 0.3645, 0.3645, 0.3270, 0.3270, 0.3902,\n",
      "          0.3902, 0.6511, 0.6511, 0.6511, 0.2803, 0.2803, 0.6971, 0.6971,\n",
      "          0.8258, 0.8258, 0.3987, 0.3987, 0.3987, 0.4971, 0.4971, 0.8356,\n",
      "          0.8356, 0.6579, 0.6579, 0.5239, 0.5239, 0.5239, 0.9727, 0.9727,\n",
      "          0.3691, 0.3691, 0.8929, 0.8929, 0.5729, 0.5729, 0.2900, 0.2900,\n",
      "          0.2900, 0.3072, 0.3072, 0.5484, 0.5484, 0.8615, 0.8615, 0.2856,\n",
      "          0.2856, 0.2856, 0.4237, 0.4237, 0.9425, 0.9425, 0.2734, 0.2734,\n",
      "          0.4255, 0.4255, 0.4255, 0.3413, 0.3413, 0.5304, 0.5304, 0.9555,\n",
      "          0.9555, 0.3440, 0.3440, 0.7646, 0.7646, 0.7646, 0.3580, 0.3580,\n",
      "          0.5307, 0.5307, 0.2062, 0.2062, 0.5297, 0.5297, 0.5297, 0.3723,\n",
      "          0.3723, 0.2611, 0.2611, 0.2148, 0.2148, 0.5071, 0.5071, 0.5071,\n",
      "          0.4321, 0.4321, 0.3498, 0.3498, 0.4510, 0.4510, 0.3434, 0.3434]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "tensor([[[0.6243, 0.6243, 0.6243, 0.4150, 0.4150, 0.5256, 0.5256, 0.7172,\n",
      "          0.7172, 0.5438, 0.5438, 0.5438, 0.2740, 0.2740, 0.2918, 0.2918,\n",
      "          0.8202, 0.8202, 0.3225, 0.3225, 0.3225, 0.8965, 0.8965, 0.6363,\n",
      "          0.6363, 0.6699, 0.6699, 0.7900, 0.7900, 0.7900, 0.3507, 0.3507,\n",
      "          0.8721, 0.8721, 0.3027, 0.3027, 0.4064, 0.4064, 0.3514, 0.3514,\n",
      "          0.3514, 0.5414, 0.5414, 0.2146, 0.2146, 0.4702, 0.4702, 0.3132,\n",
      "          0.3132, 0.3132, 0.3802, 0.3802, 0.3456, 0.3456, 0.3219, 0.3219,\n",
      "          0.6241, 0.6241, 0.6241, 0.7145, 0.7145, 0.3808, 0.3808, 0.6249,\n",
      "          0.6249, 0.6648, 0.6648, 0.3393, 0.3393, 0.3393, 0.3644, 0.3644,\n",
      "          0.5011, 0.5011, 0.3421, 0.3421, 0.5361, 0.5361, 0.5361, 0.3639,\n",
      "          0.3639, 0.5889, 0.5889, 0.3987, 0.3987, 0.7356, 0.7356, 0.7356,\n",
      "          0.3575, 0.3575, 0.2537, 0.2537, 0.8715, 0.8715, 0.2844, 0.2844,\n",
      "          0.8434, 0.8434, 0.8434, 0.3524, 0.3524, 0.3357, 0.3357, 0.5335,\n",
      "          0.5335, 0.6796, 0.6796, 0.6796, 0.7391, 0.7391, 0.4617, 0.4617,\n",
      "          0.4829, 0.4829, 0.6396, 0.6396, 0.6396, 0.4037, 0.4037, 0.6401,\n",
      "          0.6401, 0.3420, 0.3420, 0.3852, 0.3852, 0.3118, 0.3118, 0.3118,\n",
      "          0.3909, 0.3909, 0.3217, 0.3217, 0.4055, 0.4055, 0.2382, 0.2382,\n",
      "          0.2382, 0.2665, 0.2665, 0.9631, 0.9631, 0.3595, 0.3595, 0.7882,\n",
      "          0.7882, 0.7882, 0.1694, 0.1694, 0.5689, 0.5689, 0.9375, 0.9375,\n",
      "          0.9580, 0.9580, 0.9580, 0.2476, 0.2476, 0.3210, 0.3210, 0.7930,\n",
      "          0.7930, 0.6809, 0.6809, 0.2944, 0.2944, 0.2944, 0.2428, 0.2428,\n",
      "          0.3493, 0.3493, 0.2417, 0.2417, 0.3443, 0.3443, 0.3443, 0.4807,\n",
      "          0.4807, 0.2567, 0.2567, 0.3605, 0.3605, 0.3315, 0.3315, 0.3315,\n",
      "          0.3033, 0.3033, 0.9389, 0.9389, 0.6337, 0.6337, 0.5755, 0.5755,\n",
      "          0.9863, 0.9863, 0.9863, 0.5590, 0.5590, 0.3115, 0.3115, 0.8771,\n",
      "          0.8771, 0.9612, 0.9612, 0.9612, 0.2704, 0.2704, 0.4861, 0.4861,\n",
      "          0.7518, 0.7518, 0.8161, 0.8161, 0.8161, 0.4030, 0.4030, 0.5199,\n",
      "          0.5199, 0.4144, 0.4144, 0.3066, 0.3066, 0.3533, 0.3533, 0.3533,\n",
      "          0.7097, 0.7097, 0.4483, 0.4483, 0.3431, 0.3431, 0.3371, 0.3371,\n",
      "          0.3371, 0.3647, 0.3647, 0.7344, 0.7344, 0.3460, 0.3460, 0.4616,\n",
      "          0.4616, 0.4616, 0.4458, 0.4458, 0.3601, 0.3601, 0.4964, 0.4964,\n",
      "          0.4134, 0.4134, 0.3671, 0.3671, 0.3671, 0.5640, 0.5640, 0.5899,\n",
      "          0.5899, 0.6956, 0.6956, 0.5096, 0.5096, 0.5096, 0.3484, 0.3484,\n",
      "          0.5825, 0.5825, 0.5080, 0.5080, 0.4935, 0.4935, 0.4935, 0.3212,\n",
      "          0.3212, 0.3807, 0.3807, 0.3600, 0.3600, 0.3032, 0.3032, 0.3032,\n",
      "          0.5543, 0.5543, 0.3434, 0.3434, 0.4340, 0.4340, 0.3305, 0.3305,\n",
      "          0.6004, 0.6004, 0.6004, 0.3472, 0.3472, 0.3236, 0.3236, 0.5043,\n",
      "          0.5043, 0.8295, 0.8295, 0.8295, 0.3812, 0.3812, 0.4922, 0.4922,\n",
      "          0.5827, 0.5827, 0.2913, 0.2913, 0.2913, 0.2525, 0.2525, 0.3903,\n",
      "          0.3903, 0.4562, 0.4562, 0.4270, 0.4270, 0.3301, 0.3301, 0.3301,\n",
      "          0.7617, 0.7617, 0.5344, 0.5344, 0.2995, 0.2995, 0.3614, 0.3614,\n",
      "          0.3614, 0.3508, 0.3508, 0.4436, 0.4436, 0.5216, 0.5216, 0.2209,\n",
      "          0.2209, 0.2209, 0.8721, 0.8721, 0.3356, 0.3356, 0.3486, 0.3486,\n",
      "          0.3171, 0.3171, 0.4377, 0.4377, 0.4377, 0.8545, 0.8545, 0.2561,\n",
      "          0.2561, 0.3124, 0.3124, 0.5423, 0.5423, 0.5423, 0.7015, 0.7015,\n",
      "          0.6818, 0.6818, 0.3439, 0.3439, 0.5661, 0.5661, 0.5661, 0.5329,\n",
      "          0.5329, 0.5669, 0.5669, 0.2768, 0.2768, 0.2378, 0.2378, 0.3644,\n",
      "          0.3644, 0.3644, 0.3062, 0.3062, 0.2635, 0.2635, 0.5984, 0.5984,\n",
      "          0.8802, 0.8802, 0.8802, 0.4985, 0.4985, 0.7406, 0.7406, 0.5680,\n",
      "          0.5680, 0.8830, 0.8830, 0.8830, 0.6282, 0.6282, 0.3341, 0.3341,\n",
      "          0.3771, 0.3771, 0.5067, 0.5067, 0.5067, 0.4711, 0.4711, 0.2713,\n",
      "          0.2713, 0.3513, 0.3513, 0.5748, 0.5748, 0.2982, 0.2982, 0.2982,\n",
      "          0.2622, 0.2622, 0.7456, 0.7456, 0.3078, 0.3078, 0.6967, 0.6967,\n",
      "          0.6967, 0.9534, 0.9534, 0.9407, 0.9407, 0.3251, 0.3251, 0.4952,\n",
      "          0.4952, 0.4952, 0.2683, 0.2683, 0.5928, 0.5928, 0.2916, 0.2916,\n",
      "          0.2576, 0.2576, 0.8630, 0.8630, 0.8630, 0.3282, 0.3282, 0.8680,\n",
      "          0.8680, 0.5038, 0.5038, 0.3403, 0.3403, 0.3403, 0.3549, 0.3549,\n",
      "          0.3320, 0.3320, 0.5557, 0.5557, 0.3250, 0.3250, 0.3250, 0.3390,\n",
      "          0.3390, 0.6345, 0.6345, 0.3493, 0.3493, 0.2885, 0.2885, 0.4016,\n",
      "          0.4016, 0.4016, 0.8160, 0.8160, 0.3632, 0.3632, 0.3621, 0.3621,\n",
      "          0.3809, 0.3809, 0.3809, 0.2676, 0.2676, 0.3122, 0.3122, 0.2943,\n",
      "          0.2943, 0.7277, 0.7277, 0.7277, 0.7072, 0.7072, 0.1806, 0.1806,\n",
      "          0.6639, 0.6639, 0.3473, 0.3473, 0.2751, 0.2751, 0.2751, 0.4056,\n",
      "          0.4056, 0.6566, 0.6566, 0.3615, 0.3615, 0.4368, 0.4368, 0.4368,\n",
      "          0.5746, 0.5746, 0.7429, 0.7429, 0.3682, 0.3682, 0.5668, 0.5668,\n",
      "          0.5668, 0.2874, 0.2874, 0.5635, 0.5635, 0.3168, 0.3168, 0.8429,\n",
      "          0.8429, 0.8429, 0.3063, 0.3063, 0.8094, 0.8094, 0.9386, 0.9386,\n",
      "          0.4597, 0.4597, 0.2158, 0.2158, 0.2158, 0.8364, 0.8364, 0.3696,\n",
      "          0.3696, 0.6373, 0.6373, 0.7679, 0.7679, 0.7679, 0.3933, 0.3933,\n",
      "          0.3233, 0.3233, 0.4489, 0.4489, 0.5899, 0.5899, 0.5899, 0.7650,\n",
      "          0.7650, 0.7378, 0.7378, 0.8007, 0.8007, 0.2888, 0.2888, 0.3534,\n",
      "          0.3534, 0.3534, 0.3594, 0.3594, 0.4807, 0.4807, 0.6380, 0.6380,\n",
      "          0.2903, 0.2903, 0.2903, 0.4674, 0.4674, 0.8139, 0.8139, 0.3315,\n",
      "          0.3315, 0.5957, 0.5957, 0.5957, 0.3425, 0.3425, 0.4372, 0.4372,\n",
      "          0.3292, 0.3292, 0.3313, 0.3313, 0.2972, 0.2972, 0.2972, 0.3112,\n",
      "          0.3112, 0.6974, 0.6974, 0.6291, 0.6291, 0.5300, 0.5300, 0.5300,\n",
      "          0.4189, 0.4189, 0.4055, 0.4055, 0.5028, 0.5028, 0.2635, 0.2635,\n",
      "          0.2635, 0.3426, 0.3426, 0.3585, 0.3585, 0.5203, 0.5203, 0.4651,\n",
      "          0.4651, 0.5170, 0.5170, 0.5170, 0.5963, 0.5963, 0.3047, 0.3047,\n",
      "          0.4529, 0.4529, 0.4651, 0.4651, 0.4651, 0.3180, 0.3180, 0.6450,\n",
      "          0.6450, 0.1424, 0.1424, 0.2854, 0.2854, 0.2854, 0.2998, 0.2998,\n",
      "          0.5449, 0.5449, 0.3252, 0.3252, 0.7551, 0.7551, 0.7551, 0.3360,\n",
      "          0.3360, 0.8865, 0.8865, 0.6450, 0.6450, 0.7473, 0.7473, 0.8083,\n",
      "          0.8083, 0.8083, 0.4396, 0.4396, 0.4231, 0.4231, 0.4817, 0.4817,\n",
      "          0.4746, 0.4746, 0.4746, 0.6648, 0.6648, 0.3051, 0.3051, 0.2921,\n",
      "          0.2921, 0.7712, 0.7712, 0.7712, 0.7043, 0.7043, 0.7659, 0.7659,\n",
      "          0.7250, 0.7250, 0.4082, 0.4082, 0.3275, 0.3275, 0.3275, 0.7489,\n",
      "          0.7489, 0.3020, 0.3020, 0.3879, 0.3879, 0.3212, 0.3212, 0.3212,\n",
      "          0.4630, 0.4630, 0.8002, 0.8002, 0.9617, 0.9617, 0.5267, 0.5267,\n",
      "          0.5267, 0.3160, 0.3160, 0.3230, 0.3230, 0.3560, 0.3560, 0.2957,\n",
      "          0.2957, 0.5514, 0.5514, 0.5514, 0.6280, 0.6280, 0.3048, 0.3048,\n",
      "          0.6278, 0.6278, 0.4403, 0.4403, 0.4403, 0.4444, 0.4444, 0.3793,\n",
      "          0.3793, 0.3162, 0.3162, 0.5259, 0.5259, 0.5259, 0.3624, 0.3624,\n",
      "          0.4086, 0.4086, 0.3459, 0.3459, 0.3376, 0.3376, 0.5411, 0.5411,\n",
      "          0.5411, 0.2699, 0.2699, 0.2907, 0.2907, 0.2614, 0.2614, 0.3187,\n",
      "          0.3187, 0.3187, 0.3328, 0.3328, 0.3655, 0.3655, 0.2367, 0.2367,\n",
      "          0.5857, 0.5857, 0.5857, 0.6922, 0.6922, 0.3246, 0.3246, 0.5788,\n",
      "          0.5788, 0.4217, 0.4217, 0.4217, 0.3303, 0.3303, 0.7110, 0.7110,\n",
      "          0.5291, 0.5291, 0.3391, 0.3391, 0.7240, 0.7240, 0.7240, 0.6737,\n",
      "          0.6737, 0.4405, 0.4405, 0.5110, 0.5110, 0.2725, 0.2725, 0.2725,\n",
      "          0.3253, 0.3253, 0.6550, 0.6550, 0.4922, 0.4922, 0.6305, 0.6305,\n",
      "          0.6305, 0.4826, 0.4826, 0.4988, 0.4988, 0.8496, 0.8496, 0.7609,\n",
      "          0.7609, 0.3709, 0.3709, 0.3709, 0.6578, 0.6578, 0.3465, 0.3465,\n",
      "          0.3469, 0.3469, 0.2938, 0.2938, 0.2938, 0.3602, 0.3602, 0.3635,\n",
      "          0.3635, 0.2256, 0.2256, 0.3441, 0.3441, 0.3441, 0.4977, 0.4977,\n",
      "          0.2752, 0.2752, 0.3152, 0.3152, 0.2930, 0.2930, 0.8614, 0.8614,\n",
      "          0.8614, 0.9657, 0.9657, 0.5120, 0.5120, 0.6702, 0.6702, 0.7897,\n",
      "          0.7897, 0.7897, 0.2788, 0.2788, 0.2875, 0.2875, 0.3127, 0.3127,\n",
      "          0.4198, 0.4198, 0.4198, 0.6057, 0.6057, 0.3653, 0.3653, 0.4968,\n",
      "          0.4968, 0.9766, 0.9766, 0.3250, 0.3250, 0.3250, 0.3599, 0.3599,\n",
      "          0.3278, 0.3278, 0.2261, 0.2261, 0.4262, 0.4262, 0.4262, 0.5279,\n",
      "          0.5279, 0.9388, 0.9388, 0.6610, 0.6610, 0.5103, 0.5103, 0.5103,\n",
      "          0.2787, 0.2787, 0.3431, 0.3431, 0.8800, 0.8800, 0.2940, 0.2940,\n",
      "          0.2940, 0.3387, 0.3387, 0.3645, 0.3645, 0.3270, 0.3270, 0.3902,\n",
      "          0.3902, 0.6511, 0.6511, 0.6511, 0.2803, 0.2803, 0.6971, 0.6971,\n",
      "          0.8258, 0.8258, 0.3987, 0.3987, 0.3987, 0.4971, 0.4971, 0.8356,\n",
      "          0.8356, 0.6579, 0.6579, 0.5239, 0.5239, 0.5239, 0.9727, 0.9727,\n",
      "          0.3691, 0.3691, 0.8929, 0.8929, 0.5729, 0.5729, 0.2900, 0.2900,\n",
      "          0.2900, 0.3072, 0.3072, 0.5484, 0.5484, 0.8615, 0.8615, 0.2856,\n",
      "          0.2856, 0.2856, 0.4237, 0.4237, 0.9425, 0.9425, 0.2734, 0.2734,\n",
      "          0.4255, 0.4255, 0.4255, 0.3413, 0.3413, 0.5304, 0.5304, 0.9555,\n",
      "          0.9555, 0.3440, 0.3440, 0.7646, 0.7646, 0.7646, 0.3580, 0.3580,\n",
      "          0.5307, 0.5307, 0.2062, 0.2062, 0.5297, 0.5297, 0.5297, 0.3723,\n",
      "          0.3723, 0.2611, 0.2611, 0.2148, 0.2148, 0.5071, 0.5071, 0.5071,\n",
      "          0.4321, 0.4321, 0.3498, 0.3498, 0.4510, 0.4510, 0.3434, 0.3434]]],\n",
      "       grad_fn=<CloneBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [1, 32, 1], expected input[1, 1, 1000] to have 32 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m relevance_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlrp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/Xavier/Desktop/FOMOnet/prog/modules/lrp.py:22\u001b[0m, in \u001b[0;36mLRP_FOMOnet.lrp\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(relevance)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Layer-wise relevance propagation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlrp_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdconv1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrp_backward(relevance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv2)\n\u001b[1;32m     24\u001b[0m relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlrp_backward(relevance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdconv3)\n",
      "File \u001b[0;32m/mnt/c/Users/Xavier/Desktop/FOMOnet/prog/modules/lrp.py:33\u001b[0m, in \u001b[0;36mLRP_FOMOnet.lrp_backward\u001b[0;34m(self, relevance, layer)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlrp_backward\u001b[39m(\u001b[38;5;28mself\u001b[39m, relevance, layer):\n\u001b[0;32m---> 33\u001b[0m     relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlrp_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m relevance\n",
      "File \u001b[0;32m/mnt/c/Users/Xavier/Desktop/FOMOnet/prog/modules/lrp.py:37\u001b[0m, in \u001b[0;36mLRP_FOMOnet.lrp_linear\u001b[0;34m(self, layer, relevance)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlrp_linear\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer, relevance):\n\u001b[0;32m---> 37\u001b[0m     layer_out \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     layer_in \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mout_channels\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mConv1d):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [1, 32, 1], expected input[1, 1, 1000] to have 32 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "relevance_scores = model.lrp(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98b1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25ffd1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_backward(relevance, layer):\n",
    "    relevance = lrp_linear(layer, relevance)\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123de3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_linear(layer, relevance):\n",
    "    layer_out = layer(relevance)\n",
    "    layer_in = layer.out_channels\n",
    "\n",
    "    if isinstance(layer, nn.Conv1d):\n",
    "        layer_weights = layer.weight\n",
    "        layer_bias = layer.bias\n",
    "        layer_stride = layer.stride[0]\n",
    "        layer_padding = layer.padding[0]\n",
    "\n",
    "        relevance = lrp_conv1d(layer_out, relevance, layer_weights, layer_bias, layer_stride, layer_padding)\n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        layer_weights = layer.weight\n",
    "        layer_bias = layer.bias\n",
    "\n",
    "        relevance = lrp_linear_layer(layer_out, relevance, layer_weights, layer_bias)\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03b92f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_conv1d(layer_out, relevance, weights, bias, stride, padding):\n",
    "    _, _, in_length = relevance.size()\n",
    "    _, _, out_length = layer_out.size()\n",
    "\n",
    "    relevance_padded = F.pad(relevance, (padding, padding))\n",
    "    weights_flipped = torch.flip(weights, dims=[2])\n",
    "\n",
    "    unfold_relevance = F.unfold(relevance_padded, (weights.size(2),), stride=stride)\n",
    "    unfold_relevance = unfold_relevance.view(-1, in_length, weights.size(2))\n",
    "\n",
    "    unfold_relevance *= weights_flipped.unsqueeze(0)\n",
    "    unfold_relevance = unfold_relevance.sum(dim=2)\n",
    "\n",
    "    relevance = F.fold(unfold_relevance, (out_length,), (1,), stride=stride)\n",
    "\n",
    "    if bias is not None:\n",
    "        relevance += bias.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b869f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp_linear_layer(layer_out, relevance, weights, bias):\n",
    "    relevance = relevance / (layer_out + 1e-9)  # Add epsilon to avoid division by zero\n",
    "\n",
    "    relevance = relevance.matmul(weights)\n",
    "    relevance = relevance.unsqueeze(-1)\n",
    "\n",
    "    if bias is not None:\n",
    "        relevance += bias.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6611ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fceab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91af519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
